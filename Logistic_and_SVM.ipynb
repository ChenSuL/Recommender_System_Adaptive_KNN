{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from urllib import urlopen\n",
    "import scipy.optimize\n",
    "from math import exp\n",
    "from math import log\n",
    "import random\n",
    "import gzip\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# read data and meta\n",
    "def readGz(f):\n",
    "    for l in gzip.open(f):\n",
    "\t\tyield eval(l)\n",
    "\n",
    "print \"Reading data...\"\n",
    "data = json.load(open('data.json'))\n",
    "meta = json.load(open('meta.json'))\n",
    "print \"Done.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# pre-defined functions                          #\n",
    "##################################################\n",
    "def inner(x,y):\n",
    "    return sum([x[i]*y[i] for i in xrange(len(x))])\n",
    "\n",
    "def sigmoid(gamma):\n",
    "    if gamma < 0:\n",
    "        return 1 - 1/(1 + exp(gamma))\n",
    "    else:\n",
    "        return 1/(1 + exp(-gamma))\n",
    "\n",
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "    loglikelihood = 0\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        # To avoid overflow of exp(-logit), for all negative 'logit', replace '-log(1 + exp(-logit))' with 'logit - log(1 + exp(logit))'\n",
    "        if logit > 0:\n",
    "            loglikelihood -= log(1 + exp(-logit)) \n",
    "        else:\n",
    "            loglikelihood += logit - log(1 + exp(logit))\n",
    "        if not y[i]:\n",
    "            loglikelihood -= logit\n",
    "    for k in range(len(theta)):\n",
    "        loglikelihood -= lam * theta[k]*theta[k] \n",
    "    return -loglikelihood\n",
    " \n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime(theta, X, y, lam):\n",
    "    dl = [0]*len(theta)\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        for k in range(len(theta)):\n",
    "            dl[k] += X[i][k] * (1 - sigmoid(logit))\n",
    "            if not y[i]:\n",
    "                dl[k] -= X[i][k]\n",
    "    for k in range(len(theta)):\n",
    "        dl[k] -= lam*2*theta[k]\n",
    "    return numpy.array([-x for x in dl])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visited done\n",
      "unvisited done\n",
      "Training set and Test set done\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "#Prepare Training Set and Test Set               #\n",
    "##################################################\n",
    "platforms = ['VGA', 'PC', 'Linux', 'Mac', 'Nintendo', 'PlayStation', 'Sony PSP', 'Wii', 'Xbox']\n",
    "def feature(datum):\n",
    "\tl = meta[datum[1]]\n",
    "\t# Add features that whether each word in platforms appears in categories\n",
    "\tfeat = [1] + [any([1 for c in l['categories'][0] if c.startswith(p)]) for p in platforms]\n",
    "\t# Add features price/10 and salesrank\n",
    "\tfeat += [l['price']/10.0, l['salesRank'].values()[0]]\n",
    "\treturn feat\n",
    "\n",
    "# All elements in 'data' are reviews from users. Use these data as half training set that responding to y = 1. Fetch all pairs of reviewer and reviewed item from 'data' and store them in 'visited'.\n",
    "visited = set([])\n",
    "allReviewers = set([])\n",
    "for l in data:\n",
    "\tvisited.add((l['reviewerID'],l['asin']))\n",
    "\tallReviewers.add(l['reviewerID'])\n",
    "\n",
    "allReviewers = list(allReviewers)\n",
    "allItems = meta.keys()\n",
    "print \"visited done\"\n",
    "\n",
    "# To make the training set in balance, generate the data in training set that responding to y = 0. Let the number of generated datas the same with len(data). Store these pairs in 'unvisited'. \n",
    "unvisited = set()\n",
    "while len(unvisited) < len(data):\n",
    "    reviewer = allReviewers[random.randint(0,len(allReviewers)-1)]\n",
    "    item = allItems[random.randint(0,len(allItems)-1)]\n",
    "    if (reviewer, item) not in visited:\n",
    "        unvisited.add((reviewer, item))\n",
    "visited = list(visited)\n",
    "unvisited = list(unvisited)\n",
    "print \"unvisited done\"\n",
    "\n",
    "# Take the first 1/2 of visited and unvisited as the training set and the rest as test set.\n",
    "trainSet1 = visited[:len(visited)/2]\n",
    "testSet1 = visited[len(visited)/2:]\n",
    "trainSet0 = unvisited[:len(unvisited)/2]\n",
    "testSet0 = unvisited[len(unvisited)/2:]\n",
    "\n",
    "X_train = [feature(d) for d in trainSet1] + [feature(d) for d in trainSet0]\n",
    "y_train = [1 for _ in range(len(trainSet1))] + [0 for _ in range(len(trainSet0))]\n",
    "X_test = [feature(d) for d in testSet1] + [feature(d) for d in testSet0]\n",
    "y_test = [1 for _ in range(len(testSet1))] + [0 for _ in range(len(testSet0))]\n",
    "print \"Training set and Test set done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 1.0\n",
      "Accuracy on testing set by logistic regression = 0.783712028738\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# Train by logistic regression                   #\n",
    "##################################################\n",
    "lam = 1.0\n",
    "print \"lambda = \" + str(lam)\n",
    "theta,_,_ = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X_train[0]), fprime, args = (X_train, y_train, lam))\n",
    "\n",
    "# check accuracy\n",
    "predictions = [inner(theta,x) > 0 for x in X_test]\n",
    "correct = [(a==b) for (a,b) in zip(predictions, y_test)]\n",
    "acc = sum(correct) * 1.0 / len(correct)\n",
    "print \"Accuracy on testing set by logistic regression = \" + str(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing set by SVM = 0.742867854776\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# Create a support vector classifier object, with regularization parameter C = 1000\n",
    "##################################################\n",
    "clf = svm.LinearSVC(C=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# check accuracy\n",
    "test_predictions = clf.predict(X_test)\n",
    "correct = [(a==b) for (a,b) in zip(test_predictions, y_test)]\n",
    "acc = sum(correct) * 1.0 / len(correct)\n",
    "print \"Accuracy on testing set by SVM = \" + str(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KNN(object):\n",
    "\n",
    "    def __init__(self, numUsers, numItems, lamI = 6e-2, lamJ = 6e-3, learningRate = 0.1):\n",
    "        self._numUsers = numUsers\n",
    "        self._numItems = numItems\n",
    "        self._lamI = lamI\n",
    "        self._lamJ = lamJ\n",
    "        self._learningRate = learningRate\n",
    "        self._users = set()\n",
    "        self._items = set()\n",
    "        self._Iu = defaultdict(set)\n",
    "        \n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    def train(self, trainData, epochs=30, batchSize=500):\n",
    "        \n",
    "        # correlation matrix\n",
    "        self.C =np.random.rand(self._numItems, self._numItems)  \n",
    "        for l in xrange(self._numItems):\n",
    "            self.C[l][l] = 0\n",
    "            for n in xrange(l, self._numItems):\n",
    "                self.C[l][n] = self.C[n][l]\n",
    "              \n",
    "        # change batch_size to min(batch-size, len(train))\n",
    "        if len(trainData) < batchSize:\n",
    "            sys.stderr.write(\"WARNING: Batch size is greater than number of training samples, switching to a batch size of %s\\n\" % str(len(trainData)))\n",
    "            batchSize = len(trainData)\n",
    "                  \n",
    "        self._trainDict, self._users, self._items = self._dataPretreatment(trainData)\n",
    "        N = len(trainData) * epochs\n",
    "        users, pItems, nItems = self._sampling(N)\n",
    "        itr = 0\n",
    "        t2 = t0 = time.time()\n",
    "        while (itr+1)*batchSize < N:\n",
    "      \n",
    "            self._mbgd(\n",
    "                users[itr*batchSize: (itr+1)*batchSize],\n",
    "                pItems[itr*batchSize: (itr+1)*batchSize],\n",
    "                nItems[itr*batchSize: (itr+1)*batchSize]\n",
    "            )\n",
    "            \n",
    "            itr += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.3f%% ) in %.1f seconds\" %(str(itr*batchSize), 100.0 * float(itr*batchSize)/N, t2 - t0))\n",
    "            sys.stderr.flush()\n",
    "        if N > 0:\n",
    "            sys.stderr.write(\"\\nTotal training time %.2f seconds; %.2f samples per second\\n\" % (t2 - t0, N*1.0/(t2 - t0)))\n",
    "            sys.stderr.flush()\n",
    "            \n",
    "            \n",
    "    def _mbgd(self, users, pItems, nItems):\n",
    "        \n",
    "        prev = -2**10\n",
    "        for _ in xrange(30):\n",
    "            \n",
    "            gradientC = defaultdict(float)\n",
    "            obj = 0\n",
    "\n",
    "            for ind in xrange(len(users)):\n",
    "                u, i, j = users[ind], pItems[ind], nItems[ind]\n",
    "                x_ui = sum([self.C[i][l] for l in self._Iu[u] if i != l])\n",
    "                x_uj = sum([self.C[j][l] for l in self._Iu[u]])\n",
    "                x_uij =  x_ui - x_uj\n",
    "                \n",
    "                for l in self._Iu[u]:\n",
    "                    if l != i:\n",
    "                        gradientC[(i,l)] += (1-self.sigmoid(x_uij)) + self._lamI * self.C[i][l]**2\n",
    "                        gradientC[(l,i)] += (1-self.sigmoid(x_uij)) + self._lamI * self.C[l][i]**2\n",
    "                    gradientC[(j,l)] += -(1-self.sigmoid(x_uij)) + self._lamJ * self.C[j][l]**2\n",
    "                    gradientC[(l,j)] += -(1-self.sigmoid(x_uij)) + self._lamJ * self.C[l][j]**2\n",
    "                    \n",
    "                    obj -= 2*self._lamI * self.C[i][l]**2 + 2*self._lamJ * self.C[j][l]**2\n",
    "                    \n",
    "                obj += log(self.sigmoid(x_uij))\n",
    "            \n",
    "            #print 'OBJ: ', obj\n",
    "            if prev > obj: \n",
    "                break\n",
    "            prev = obj\n",
    "            \n",
    "            for a,b in gradientC:\n",
    "                self.C[a][b] += self._learningRate * gradientC[(a,b)]\n",
    "            \n",
    "        #print _, '\\n'\n",
    "        \n",
    "    def _sampling(self, N):\n",
    "        \n",
    "        sys.stderr.write(\"Generating %s random training samples\\n\" % str(N))\n",
    "        userList = list(self._users)\n",
    "        userIndex = np.random.randint(0, len(self._users), N)\n",
    "        pItems, nItems = [], []\n",
    "        cnt = 0\n",
    "        for index in userIndex:\n",
    "            u = userList[index]\n",
    "            i = self._trainDict[u][np.random.randint(len(self._Iu[u]))]\n",
    "            pItems.append(i)\n",
    "            j = np.random.randint(self._numItems)\n",
    "            while j in self._Iu[u]:\n",
    "                j = np.random.randint(self._numItems)\n",
    "            nItems.append(j)\n",
    "            \n",
    "            cnt += 1\n",
    "            if not cnt %10000:\n",
    "                sys.stderr.write(\"\\rGenerated %s\" %(str(cnt)))\n",
    "                sys.stderr.flush()\n",
    "        return userIndex, pItems, nItems\n",
    "\n",
    "    def predictionsKNN(self, K, u):\n",
    "        #slow\n",
    "        if K >= self._Iu[u]:\n",
    "            res = np.sum([self.C[:,l] for l in self._Iu[u]], 0)\n",
    "        else:\n",
    "            res = []\n",
    "            for i in xrange(self._numItems):\n",
    "                res.append(sum(sorted([self.C[i][l] for l in self._Iu[u]], reverse=True)[:K]))\n",
    "        return res\n",
    "\n",
    "    def predictionsAll(self, u):\n",
    "        \n",
    "        res = np.sum([self.C[:,l] for l in self._Iu[u]], 0)\n",
    "        return res\n",
    "\n",
    "    def prediction(self, u, i):\n",
    "        \n",
    "        scores = self.predictions(u)\n",
    "        return scores[i] > sorted(scores)[self._numItem*0.8]\n",
    "\n",
    "    def _dataPretreatment(self, data):\n",
    "        dataDict = defaultdict(list)\n",
    "        items = set()\n",
    "        for u, i in data:\n",
    "            self._Iu[u].add(i)\n",
    "            dataDict[u].append(i)\n",
    "            items.add(i)\n",
    "        return dataDict, set(dataDict.keys()), items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(data)/2\n",
    "users = set()\n",
    "items = set()\n",
    "visited = set()\n",
    "businessCount = defaultdict(int)\n",
    "\n",
    "for l in data[:length]:\n",
    "    user,business = l['reviewerID'],l['asin']\n",
    "    users.add(user)  \n",
    "    items.add(business)\n",
    "    visited.add((user, business))  #visited pair\n",
    "    businessCount[business] += 1\n",
    "    \n",
    "mostPopular = [(businessCount[x], x) for x in businessCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count*1.0/length > 0.571: \n",
    "        break\n",
    "\n",
    "users = list(users)\n",
    "items = list(items)\n",
    "Iu = defaultdict(set)\n",
    "Ui = defaultdict(set)\n",
    "for l in data[:length]:\n",
    "    Iu[l['reviewerID']].add(l['asin'])\n",
    "    Ui[l['asin']].add(l['reviewerID'])\n",
    "    \n",
    "users = {value:key for key, value in enumerate(users)}\n",
    "items = {value:key for key, value in enumerate(items)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114692"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testSet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating 573460 random training samples\n",
      "Processed 573000 ( 99.920% ) in 109.2 seconds\n",
      "Total training time 109.20 seconds; 5251.31 samples per second\n"
     ]
    }
   ],
   "source": [
    "train = [(users[l['reviewerID']], items[l['asin']]) for l in data[:length]]   \n",
    "bpr = KNN(len(users), len(items), lamI = 4e-2, lamJ = 4e-3, learningRate = 0.09)\n",
    "bpr.train(train, epochs=5, batchSize=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed 229376"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "cnt = 0\n",
    "for u,i in testSet1 + testSet0:\n",
    "    cnt += 1\n",
    "    if u not in users:  #no history of user, use popularity\n",
    "        if i in return1:\n",
    "            res.append(1)\n",
    "        else:\n",
    "            res.append(0)\n",
    "        continue\n",
    "        \n",
    "    if i not in items:   #no one visited before\n",
    "        res.append(0)\n",
    "        continue\n",
    "        \n",
    "    scores = bpr.predictionsKNN(10, users[u])\n",
    "    #scores = bpr.predictionsAll(users[u])\n",
    "    #thrhd = sorted(scores)[len(items)*7/10]\n",
    "    score = scores[items[i]]\n",
    "    \n",
    "    num = len([1 for x in scores if x > score])\n",
    "\n",
    "    #if score >= thrhd:\n",
    "    if num < len(scores)*30/100:\n",
    "        res.append(1)\n",
    "    else:\n",
    "        res.append(0)\n",
    "        \n",
    "    sys.stderr.write(\"\\rProcessed %s\" %(str(cnt)))\n",
    "    sys.stderr.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing set by knn = 0.731554947163\n"
     ]
    }
   ],
   "source": [
    "correct = [(a==b) for (a,b) in zip(res, y_test)]\n",
    "acc = sum(correct) * 1.0 / len(correct)\n",
    "print \"Accuracy on testing set by knn = \" + str(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "ERROR (theano.gpuarray): Could not initialize pygpu, support disabled\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sun/anaconda2/lib/python2.7/site-packages/theano/gpuarray/__init__.py\", line 164, in <module>\n",
      "    use(config.device)\n",
      "  File \"/home/sun/anaconda2/lib/python2.7/site-packages/theano/gpuarray/__init__.py\", line 151, in use\n",
      "    init_dev(device)\n",
      "  File \"/home/sun/anaconda2/lib/python2.7/site-packages/theano/gpuarray/__init__.py\", line 47, in init_dev\n",
      "    raise RuntimeError(\"The new gpu-backend need a c++ compiler.\")\n",
      "RuntimeError: The new gpu-backend need a c++ compiler.\n",
      "Generating 229384 random training samples\n",
      "Processed 229000 ( 99.83% ) in 1.4997 seconds\n",
      "Total training time 695.72 seconds; 3.032985e-03 per sample\n"
     ]
    }
   ],
   "source": [
    "from theano_bpr import BPR\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano import function, config, shared, sandbox  \n",
    "\n",
    "bpr = BPR(5, len(users), len(items), lambda_u = 1e-3, lambda_i = 1e-3, lambda_j = 1e-3, lambda_bias = 1)\n",
    "bpr.train(train, epochs=2, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed 229376"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing set by MF = 0.792077913019\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "cnt = 0\n",
    "for u,i in testSet1 + testSet0:\n",
    "    cnt += 1\n",
    "    if u not in users:  #no history of user, use popularity\n",
    "        if i in return1:\n",
    "            res.append(1)\n",
    "        else:\n",
    "            res.append(0)\n",
    "        continue\n",
    "        \n",
    "    if i not in items:   #no one visited before\n",
    "        res.append(0)\n",
    "        continue\n",
    "        \n",
    "    scores = bpr.predictions(users[u])\n",
    "    #scores = bpr.predictionsAll(users[u])\n",
    "    #thrhd = sorted(scores)[len(items)*7/10]\n",
    "    score = scores[items[i]]\n",
    "    \n",
    "    num = len([1 for x in scores if x > score])\n",
    "\n",
    "    #if score >= thrhd:\n",
    "    if num < len(scores)*30/100:\n",
    "        res.append(1)\n",
    "    else:\n",
    "        res.append(0)\n",
    "        \n",
    "    sys.stderr.write(\"\\rProcessed %s\" %(str(cnt)))\n",
    "    sys.stderr.flush()\n",
    "correct = [(a==b) for (a,b) in zip(res, y_test)]\n",
    "acc = sum(correct) * 1.0 / len(correct)\n",
    "print \"Accuracy on testing set by MF = \" + str(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'asin': u'B0018YXM3Y',\n",
       " u'helpful': [48, 78],\n",
       " u'overall': 5.0,\n",
       " u'reviewText': u'I have been a huge fan of the Total War series from Creative Assembly, first starting with Shogun nearly a decade ago, then moving onto both Medieval Total Wars, as well as Rome Total War.  However, I have not been nearly as excited about those games as I have been about Empire.  The time period is fascinating, and the warfare spectacular.  I think many a history fan has long imagined himself (or herself) as Major Pitcairn, at the head of a column of redcoats, facing down a ragtag bunch of insolent tea-dumping rebels, or as the Comte de Conflans, at the head of a battle line entering Quiberon Bay amidst a howling gale, exchanging broadsides with the British fleet.  I remember playing MicroProse\\'s Napoleonic 2-D RTS game, Fields of Glory, with British troops marching into battle with the cheerful Grenadier March playing, or the French Imperial Guard with Victory at Hand playing mightily.  How the times have changed!  Now with Empire, you can play massive battles with thousands of men at your disposal, flags fluttering, drums thumping, and men charging, all to a visually spectacular 3-D.Graphics wise, this game is amazing.  You can see men scurrying around on ship decks manning their carronades, marines on fighting tops shooting downwards.  There is quite a bit of battle animation built into the game as well.  Men load their rifles, insert the ramrods, parry bayonets, shoulder their rifles, and charge into battle.  Gunners sponge their cannons, ignite the touch hole, and crouch as their pieces belch forth death and fire.  Infantry can setup defensive wooden palings to guard against cavalry charges.  The water detail is impressive, with reflections and beautiful environmental effects.Speaking of graphics, I believe the minimum requirements are a single core machine with a 256 MB video card.  You should not heed those minimum requirements, unless you wish to play on all low resolutions at a few frames per second.  You need no less than a Core 2 Duo, preferably Core 2 Quad or Core i7 for best results, with at least a 512 MB video card.  This game shines on Very High/Ultra settings, and to play at lower resolutions would truly be a disservice to yourself, and the hard work the game designers invested.  For those of you with monster rigs, you can increase unit sizes to maximize battle carnage.Here\\'s a rough idea of the level of detail you can use with the following video cards.  Note, that at this time, there are graphical issues with using ATI video cards (NVIDIA cards are fine) which may or may not be resolved in an upcoming patch.ETW- Ultra:Nvidia 9800GT/GTX/GX2Nvidia GTX 260/280ATI HD 4850/4870ETW- High:Nvidia 8800 GTX/GT/GTS 640Nvidia 9600ATI HD 3850ETW- Medium:Nvidia 8800 GTS 320ETW- Low:Nvidia 8600Nvidia 8500Nvidia 9500Nvidia 9400I feel as I should also address that you must have STEAM to run this game.  I don\\'t have any issues with Steam, it\\'s less intrusive than other DRM measures, and it\\'s convenient.  I\\'ve been using Steam for years, and it\\'s a proven distribution system.As mentioned, yes, there are naval battles, finally!  Your ships can fire 3 types of shot- regular round shot, chain shot, and grape.  It takes a bit of practice to get control of your fleet, and with more massive fleets, this requires some work.  However, there is nothing more satisfying than crossing your enemy\\'s T and firing broadsides into him.  Boarding ships is a risky proposition that may damage your own ship; instead, try firing chain shot to disrupt the enemy\\'s formation, and force ships out of line.  Land battles add an option of selecting formations, which can improve your tactical success.  Carefully choose how to group your infantry, cavalry, and artillery, and victory may be yours.  Seeing your entire infantry line erupt in a single line of musket fire at opposing forces is a sight to behold.  I was rather disappointed that there are no bagpipes/fifes to authenticate the atmosphere.  There is a drumbeat for marching infantry, and bugle calls for cavalry, but sadly no Grenadier\\'s March accompanying the troops (that I could hear).The AI is vastly improved over previous Total War installments.  The AI will react if you move on its flanks, and is quite tough to beat on the hard level.  There are some quirks though, where the AI troops will stand in place while your cannons tear them to shreds.  Sieges are not quite as tedious as before, as the AI will actually move to attack your fort.  However, sieges still have issues, and I prefer to let the computer resolve them.  Path-finding is still a big problem, and can detract from gameplay.If you haven\\'t been a close follower of the Total War series, then there is a bit of a learning curve, but it\\'s well worth the time of learn the game.  The overall tactical map with displayed countries and turn-based movements is by and large the same.  However, there is much more detail on the map, the map is gigantic, and you can espy neat little animations.  It\\'s easier to keep track of your citizens\\' happiness levels now, although I still find taxes a bit confusing.  Research is a new feature added by Creative Assembly, and you must carefully choose which categories (military, industry, or philosophy) you wish to study.  Playable nations include Britain, France, the US Colonies, Sweden, United Provinces (Netherlands).  Historical figures include the Duke of Marlborough, Peter the Great, and Charles XII of Sweden.  Continents span Europe, North and Central America, as well as India.PROS: Early Modern Europe modeled in 3-D goodness, stable buildCONS: No bagpipes/fifes, no POW taking, some path-finding issues, minor AI problems, load times (though improved from the demo), sieges improved but still niggling issues.Now go get the game and send your minions on a frontal attack!*** Don\\'t forget: Press the \"Insert\" key in-game to get a first-person camera angle.  It\\'s awesome!',\n",
       " u'reviewTime': u'03 3, 2009',\n",
       " u'reviewerID': u'A3EFSLEMHNPP6A',\n",
       " u'reviewerName': u'Senor Zoidbergo',\n",
       " u'summary': u'Why are you wasting time reading this review?  Go and buy the game!',\n",
       " u'unixReviewTime': 1236038400}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
